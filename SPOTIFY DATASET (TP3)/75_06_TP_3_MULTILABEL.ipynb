{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Za3yIbYbyYU_ZVYzNUqDXnQHQ_VzY1LO",
      "authorship_tag": "ABX9TyOZMFw2IazqnRAEqbAYLscQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbarreto10/data-science-2022/blob/main/SPOTIFY%20DATASET%20(TP3)/75_06_TP_3_MULTILABEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK4KbF68fMDd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f30328-2b1f-4c40-a46f-a06696103b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "pd.options.mode.chained_assignment = None\n",
        "!pip install scikit-multilearn\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LECTURA**"
      ],
      "metadata": {
        "id": "Yk8HjbiWqXxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf = pd.read_parquet(\"/content/drive/MyDrive/tp3/train.parquet\")\n",
        "testDf = pd.read_parquet(\"/content/drive/MyDrive/tp3/test.parquet\")"
      ],
      "metadata": {
        "id": "RdbFhCTdqUxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf[\"genre\"][trainDf[trainDf.genre == \"Children's Music\"].index[0]] = \"Children’s Music\""
      ],
      "metadata": {
        "id": "dVXVj9_ewDDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_label(df, fitting=[]):\n",
        "    grp = df[[\"track_name\",\"artist\",\"genre\"]]\n",
        "    grp[\"1\"] = 1\n",
        "    grp[\"id\"] = grp.apply(lambda x: (x.track_name, x.artist), axis=1)\n",
        "    piv = grp.pivot_table(columns=\"genre\", index=\"id\", values=\"1\", fill_value=0)\n",
        "    for g in fitting:\n",
        "        if g not in piv.columns:\n",
        "            piv[g] = 0\n",
        "    df[\"id\"] = df.apply(lambda x: (x.track_name, x.artist), axis=1)\n",
        "    df = df.drop_duplicates(subset=[\"id\"], keep='first')\n",
        "\n",
        "    return df.merge(piv,on=\"id\"), piv.columns"
      ],
      "metadata": {
        "id": "C2wKPH1BwXhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf, trainGenres = multi_label(trainDf)\n",
        "testDf, genreList = multi_label(testDf, fitting=trainGenres)\n",
        "genreList = genreList.tolist()"
      ],
      "metadata": {
        "id": "uZVM9JTAnn9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SPLIT**"
      ],
      "metadata": {
        "id": "Xz-fcUe-qcLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(3)\n",
        "artistList = trainDf.artist.unique().tolist()\n",
        "validationArtists = random.sample(artistList, int(0.2*len(artistList)))\n",
        "trainDf, valDf = trainDf.query(\"artist not in @validationArtists\"), trainDf.query(\"artist in @validationArtists\")"
      ],
      "metadata": {
        "id": "oxRU_1Buqcgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPROCESAMIENTO**"
      ],
      "metadata": {
        "id": "z1FDAYqkqfla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwordsSpEn = set(stopwords.words('english')+stopwords.words('spanish'))\n",
        "sPunctuations = list(string.punctuation)\n",
        "sDigits = list(string.digits)"
      ],
      "metadata": {
        "id": "zjXID-VF6jmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_type_count(x, tset):\n",
        "    words = x.split()\n",
        "    return len([s for s in words if s in tset])\n",
        "\n",
        "def digit_count(x):\n",
        "    return len([d for d in x if d in sDigits])\n",
        "\n",
        "def word_max(x):\n",
        "    words = word_tokenize(x)\n",
        "    return 0 if len(words)==0 else max([len(w) for w in words])\n",
        "\n",
        "def word_min(x):\n",
        "    words = word_tokenize(x)\n",
        "    return 0 if len(words)==0 else min([len(w) for w in words if w not in stopwordsSpEn])\n",
        "\n",
        "def word_mean(x):\n",
        "    words = word_tokenize(x)\n",
        "    return 0 if len(words)==0 else np.mean([len(w) for w in words])\n",
        "\n",
        "def sent_max(x):\n",
        "    sents = sent_tokenize(x)\n",
        "    return 0 if len(sents)==0 else max([len(s) for s in sents])\n",
        "\n",
        "def sent_min(x):\n",
        "    sents = sent_tokenize(x)\n",
        "    return 0 if len(sents)==0 else min([len(s) for s in sents])\n",
        "\n",
        "def sent_mean(x):\n",
        "    sents = sent_tokenize(x)\n",
        "    return 0 if len(sents)==0 else np.mean([len(s) for s in sents])\n",
        "\n",
        "def preprocess(df):\n",
        "    # DROPEO\n",
        "    df = df.drop(columns = [\"id\",\"track_name\",\"did\",\"artist\",\"a_genres\",\"a_songs\"])\n",
        "\n",
        "    # IMPUTACIÓN DE NULOS\n",
        "    s_labelMean = df[\"s-label\"].mean()\n",
        "    df.lyric = df.lyric.fillna(\"\").astype(str)\n",
        "    df.language = df.language.map(lambda x: \"ot\" if type(x)==type(None) else x)\n",
        "    df[\"s-label\"] = df[\"s-label\"].fillna(s_labelMean)\n",
        "    df[\"mode\"] = df[\"mode\"].map(lambda m: int(m==\"Major\"))\n",
        "\n",
        "    # CREACIÓN DE FEATURES A PARTIR DE LAS LYRICS\n",
        "    df[\"lyricCharCount\"] = df.lyric.map(len)\n",
        "    df[\"lyricWordCount\"] = df.lyric.map(lambda x: len(word_tokenize(x)))\n",
        "    df[\"lyricUniqueWordCount\"] = df.lyric.map(lambda x: len(set(x.split())))\n",
        "    df[\"lyricSentenceCount\"] = df.lyric.map(lambda x: len(sent_tokenize(x)))\n",
        "    df[\"lyricUniqueSentenceCount\"] = df.lyric.map(lambda x: len(set(sent_tokenize(x))))\n",
        "    df[\"lyricDigitCount\"] = df.lyric.map(digit_count)\n",
        "    df[\"lyricStopwordCount\"] = df.lyric.map(lambda x: str_type_count(x, stopwordsSpEn))\n",
        "    df[\"lyricPunctuationCount\"] = df.lyric.map(lambda x: str_type_count(x, sPunctuations))\n",
        "    df[\"lyricLongestWordLen\"] = df.lyric.map(word_max)\n",
        "    df[\"lyricShortestWordLen\"] = df.lyric.map(word_min)\n",
        "    df[\"lyricWordLenMean\"] = df.lyric.map(word_mean)\n",
        "    df[\"lyricLongestSentenceLen\"] = df.lyric.map(sent_max)\n",
        "    df[\"lyricShortestSentenceLen\"] = df.lyric.map(sent_min)\n",
        "    df[\"lyricSentenceLenMean\"] = df.lyric.map(sent_mean)\n",
        "\n",
        "    #Mean encoding de a_popularity respecto de categóricas\n",
        "    grpByLanguage = df.groupby([\"language\"]).mean()[\"a_popularity\"]\n",
        "    grpByKey = df.groupby([\"key\"]).mean()[\"a_popularity\"]\n",
        "    grpByTimeSignature = df.groupby([\"time_signature\"]).mean()[\"a_popularity\"]\n",
        "    grpByMode = df.groupby([\"mode\"]).mean()[\"a_popularity\"]\n",
        "    df[\"a_popularityMeanByLanguage\"] = df[\"language\"].map(lambda x: grpByLanguage[x])\n",
        "    df[\"a_popularityMeanByKey\"] = df[\"key\"].map(lambda x: grpByKey[x])\n",
        "    df[\"a_popularityMeanByTimeSignature\"] = df[\"time_signature\"].map(lambda x: grpByTimeSignature[x])\n",
        "    df[\"a_popularityMeanByMode\"] = df[\"mode\"].map(lambda x: grpByMode[x])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Qbxl37RIqx3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf = preprocess(trainDf)\n",
        "valDf = preprocess(valDf)\n",
        "testDf = preprocess(testDf)"
      ],
      "metadata": {
        "id": "TeIJh0Jo4058"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized(column):\n",
        "    colStd = column.std()\n",
        "    return (column - column.mean()) / colStd if colStd!=0 else 0 * column\n",
        "\n",
        "def normalize_df(df, featureList):\n",
        "    for f in featureList:\n",
        "        df[f] = normalized(df[f])\n",
        "    return df\n",
        "\n",
        "def one_hot_encode(df, catCols):\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "    encoder.fit(df[catCols])\n",
        "    encFts = list(encoder.get_feature_names(catCols))\n",
        "    df[encFts] = encoder.transform(df[catCols])\n",
        "    return df"
      ],
      "metadata": {
        "id": "AKyIyiZhAmIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catCols = [\"language\",\"time_signature\",\"key\"]\n",
        "trainDf = one_hot_encode(trainDf, catCols)\n",
        "valDf = one_hot_encode(valDf, catCols)\n",
        "testDf = one_hot_encode(testDf, catCols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGGJvW8pBnrN",
        "outputId": "29fcec2c-b009-40e2-c1c4-5b135d809170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_nan_lyrics(df):\n",
        "    df.lyric = df.apply(lambda x: \"\" if type(x.lyric)!=str else x.lyric, axis=1)\n",
        "    return df"
      ],
      "metadata": {
        "id": "KJ0vYQGNzKk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf = fix_nan_lyrics(trainDf)\n",
        "valDf = fix_nan_lyrics(valDf)\n",
        "testDf = fix_nan_lyrics(testDf)"
      ],
      "metadata": {
        "id": "opaGdY5G2Sbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "countIDF = CountVectorizer(lowercase=True, stop_words=stopwordsSpEn, max_features=50)\n",
        "countIDF.fit(trainDf.lyric)\n",
        "ftNames = [\"word\"+w.capitalize() for w in countIDF.get_feature_names_out()]\n",
        "\n",
        "for df in [trainDf, valDf, testDf]:\n",
        "    wordMatrix = countIDF.transform(df.lyric)\n",
        "    df[ftNames] = pd.DataFrame(wordMatrix.todense(), columns=ftNames, index=df.index)"
      ],
      "metadata": {
        "id": "_9f8bSJKx7zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featureList = [f for f in trainDf if f not in genreList+[\"id\",\"genre\",\"lyric\",\"language\",\"time_signature\",\"key\"]]"
      ],
      "metadata": {
        "id": "NOXbwJ5gAmMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HAY CATEGORÍAS EN EL TRAIN_DF QUE NO ESTÁN EN EL VAL_DF O EN EL TEST_DF\n",
        "# COMO EL ONEHOTENCODING NO LAS CREO, LAS CREO COMO NULAS\n",
        "for f in featureList:\n",
        "    if f not in valDf:\n",
        "        valDf[f] = 0\n",
        "    if f not in testDf:\n",
        "        testDf[f] = 0"
      ],
      "metadata": {
        "id": "uKDSFY6eB0Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf = normalize_df(trainDf, featureList)\n",
        "valDf = normalize_df(valDf, featureList)\n",
        "testDf = normalize_df(testDf, featureList)"
      ],
      "metadata": {
        "id": "Ns9winN8BjQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = trainDf[featureList], trainDf[genreList]\n",
        "X_test, y_test = testDf[featureList], testDf[genreList]\n",
        "X_val, y_val = valDf[featureList], valDf[genreList]"
      ],
      "metadata": {
        "id": "RkivO-C5B4h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ESTIMADOR MULTILABEL**"
      ],
      "metadata": {
        "id": "6Ilt_7kWCpmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "metadata": {
        "id": "iE3KxXe3CpIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgbEstimator = XGBClassifier(objective='binary:logistic', seed=10)\n",
        "multilabelModel = MultiOutputClassifier(xgbEstimator)\n",
        "multilabelModel.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPwCGYIqC-gX",
        "outputId": "a4ec69c2-80cb-4428-d835-f23a365b1e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiOutputClassifier(estimator=XGBClassifier(seed=10))"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TUNEO DEL MODELO\n",
        "hpar_space = [\n",
        "    {\n",
        "        \"estimator\": [xgbEstimator],\n",
        "        \"estimator__eta\": np.arange(0, 0.5, 0.025),\n",
        "        \"estimator__max_depth\": np.arange(3, 10),\n",
        "        \"estimator__n_estimators\": np.arange(3, 15),\n",
        "        \"estimator__min_child_weight\": np.arange(0, 10)\n",
        "    },\n",
        "]\n",
        "\n",
        "search = RandomizedSearchCV(multilabelModel, hpar_space, cv=3, n_iter=12, scoring='accuracy', verbose=10, random_state=420)\n",
        "result = search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "PuncOr9wovho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "633ec181-324f-4840-ffa1-62113f02284c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "[CV 1/3; 1/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=4\n",
            "[CV 1/3; 1/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=4;, score=0.083 total time=   5.7s\n",
            "[CV 2/3; 1/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=4\n",
            "[CV 2/3; 1/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=4;, score=0.144 total time=   5.4s\n",
            "[CV 3/3; 1/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=4\n",
            "[CV 3/3; 1/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=4;, score=0.141 total time=   5.4s\n",
            "[CV 1/3; 2/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.4, estimator__max_depth=9, estimator__min_child_weight=7, estimator__n_estimators=13\n",
            "[CV 1/3; 2/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.4, estimator__max_depth=9, estimator__min_child_weight=7, estimator__n_estimators=13;, score=0.112 total time=  19.1s\n",
            "[CV 2/3; 2/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.4, estimator__max_depth=9, estimator__min_child_weight=7, estimator__n_estimators=13\n",
            "[CV 2/3; 2/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.4, estimator__max_depth=9, estimator__min_child_weight=7, estimator__n_estimators=13;, score=0.140 total time=  18.6s\n",
            "[CV 3/3; 2/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.4, estimator__max_depth=9, estimator__min_child_weight=7, estimator__n_estimators=13\n",
            "[CV 3/3; 2/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.4, estimator__max_depth=9, estimator__min_child_weight=7, estimator__n_estimators=13;, score=0.177 total time=  18.2s\n",
            "[CV 1/3; 3/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.025, estimator__max_depth=6, estimator__min_child_weight=0, estimator__n_estimators=12\n",
            "[CV 1/3; 3/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.025, estimator__max_depth=6, estimator__min_child_weight=0, estimator__n_estimators=12;, score=0.095 total time=  14.4s\n",
            "[CV 2/3; 3/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.025, estimator__max_depth=6, estimator__min_child_weight=0, estimator__n_estimators=12\n",
            "[CV 2/3; 3/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.025, estimator__max_depth=6, estimator__min_child_weight=0, estimator__n_estimators=12;, score=0.153 total time=  13.8s\n",
            "[CV 3/3; 3/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.025, estimator__max_depth=6, estimator__min_child_weight=0, estimator__n_estimators=12\n",
            "[CV 3/3; 3/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.025, estimator__max_depth=6, estimator__min_child_weight=0, estimator__n_estimators=12;, score=0.152 total time=  13.8s\n",
            "[CV 1/3; 4/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=5, estimator__min_child_weight=6, estimator__n_estimators=5\n",
            "[CV 1/3; 4/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=5, estimator__min_child_weight=6, estimator__n_estimators=5;, score=0.086 total time=   6.3s\n",
            "[CV 2/3; 4/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=5, estimator__min_child_weight=6, estimator__n_estimators=5\n",
            "[CV 2/3; 4/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=5, estimator__min_child_weight=6, estimator__n_estimators=5;, score=0.139 total time=   6.2s\n",
            "[CV 3/3; 4/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=5, estimator__min_child_weight=6, estimator__n_estimators=5\n",
            "[CV 3/3; 4/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=5, estimator__min_child_weight=6, estimator__n_estimators=5;, score=0.160 total time=   6.1s\n",
            "[CV 1/3; 5/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=5\n",
            "[CV 1/3; 5/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=5;, score=0.081 total time=   6.3s\n",
            "[CV 2/3; 5/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=5\n",
            "[CV 2/3; 5/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=5;, score=0.144 total time=   6.3s\n",
            "[CV 3/3; 5/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=5\n",
            "[CV 3/3; 5/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=5, estimator__min_child_weight=2, estimator__n_estimators=5;, score=0.143 total time=   6.2s\n",
            "[CV 1/3; 6/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=9, estimator__min_child_weight=8, estimator__n_estimators=10\n",
            "[CV 1/3; 6/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=9, estimator__min_child_weight=8, estimator__n_estimators=10;, score=0.098 total time=  16.2s\n",
            "[CV 2/3; 6/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=9, estimator__min_child_weight=8, estimator__n_estimators=10\n",
            "[CV 2/3; 6/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=9, estimator__min_child_weight=8, estimator__n_estimators=10;, score=0.143 total time=  14.6s\n",
            "[CV 3/3; 6/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=9, estimator__min_child_weight=8, estimator__n_estimators=10\n",
            "[CV 3/3; 6/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.30000000000000004, estimator__max_depth=9, estimator__min_child_weight=8, estimator__n_estimators=10;, score=0.177 total time=  14.4s\n",
            "[CV 1/3; 7/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.275, estimator__max_depth=4, estimator__min_child_weight=7, estimator__n_estimators=5\n",
            "[CV 1/3; 7/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.275, estimator__max_depth=4, estimator__min_child_weight=7, estimator__n_estimators=5;, score=0.062 total time=   5.6s\n",
            "[CV 2/3; 7/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.275, estimator__max_depth=4, estimator__min_child_weight=7, estimator__n_estimators=5\n",
            "[CV 2/3; 7/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.275, estimator__max_depth=4, estimator__min_child_weight=7, estimator__n_estimators=5;, score=0.137 total time=   5.5s\n",
            "[CV 3/3; 7/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.275, estimator__max_depth=4, estimator__min_child_weight=7, estimator__n_estimators=5\n",
            "[CV 3/3; 7/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.275, estimator__max_depth=4, estimator__min_child_weight=7, estimator__n_estimators=5;, score=0.149 total time=   5.5s\n",
            "[CV 1/3; 8/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=9, estimator__min_child_weight=6, estimator__n_estimators=14\n",
            "[CV 1/3; 8/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=9, estimator__min_child_weight=6, estimator__n_estimators=14;, score=0.107 total time=  20.8s\n",
            "[CV 2/3; 8/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=9, estimator__min_child_weight=6, estimator__n_estimators=14\n",
            "[CV 2/3; 8/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=9, estimator__min_child_weight=6, estimator__n_estimators=14;, score=0.152 total time=  20.2s\n",
            "[CV 3/3; 8/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=9, estimator__min_child_weight=6, estimator__n_estimators=14\n",
            "[CV 3/3; 8/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=9, estimator__min_child_weight=6, estimator__n_estimators=14;, score=0.173 total time=  19.8s\n",
            "[CV 1/3; 9/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=4, estimator__min_child_weight=2, estimator__n_estimators=11\n",
            "[CV 1/3; 9/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=4, estimator__min_child_weight=2, estimator__n_estimators=11;, score=0.069 total time=   9.8s\n",
            "[CV 2/3; 9/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=4, estimator__min_child_weight=2, estimator__n_estimators=11\n",
            "[CV 2/3; 9/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=4, estimator__min_child_weight=2, estimator__n_estimators=11;, score=0.144 total time=   9.5s\n",
            "[CV 3/3; 9/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=4, estimator__min_child_weight=2, estimator__n_estimators=11\n",
            "[CV 3/3; 9/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.0, estimator__max_depth=4, estimator__min_child_weight=2, estimator__n_estimators=11;, score=0.146 total time=   9.4s\n",
            "[CV 1/3; 10/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=4, estimator__min_child_weight=4, estimator__n_estimators=8\n",
            "[CV 1/3; 10/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=4, estimator__min_child_weight=4, estimator__n_estimators=8;, score=0.069 total time=   7.6s\n",
            "[CV 2/3; 10/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=4, estimator__min_child_weight=4, estimator__n_estimators=8\n",
            "[CV 2/3; 10/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=4, estimator__min_child_weight=4, estimator__n_estimators=8;, score=0.141 total time=   7.4s\n",
            "[CV 3/3; 10/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=4, estimator__min_child_weight=4, estimator__n_estimators=8\n",
            "[CV 3/3; 10/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.325, estimator__max_depth=4, estimator__min_child_weight=4, estimator__n_estimators=8;, score=0.152 total time=   7.4s\n",
            "[CV 1/3; 11/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.07500000000000001, estimator__max_depth=9, estimator__min_child_weight=1, estimator__n_estimators=5\n",
            "[CV 1/3; 11/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.07500000000000001, estimator__max_depth=9, estimator__min_child_weight=1, estimator__n_estimators=5;, score=0.101 total time=   9.4s\n",
            "[CV 2/3; 11/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.07500000000000001, estimator__max_depth=9, estimator__min_child_weight=1, estimator__n_estimators=5\n",
            "[CV 2/3; 11/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.07500000000000001, estimator__max_depth=9, estimator__min_child_weight=1, estimator__n_estimators=5;, score=0.147 total time=   9.4s\n",
            "[CV 3/3; 11/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.07500000000000001, estimator__max_depth=9, estimator__min_child_weight=1, estimator__n_estimators=5\n",
            "[CV 3/3; 11/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.07500000000000001, estimator__max_depth=9, estimator__min_child_weight=1, estimator__n_estimators=5;, score=0.152 total time=   9.1s\n",
            "[CV 1/3; 12/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.17500000000000002, estimator__max_depth=8, estimator__min_child_weight=9, estimator__n_estimators=7\n",
            "[CV 1/3; 12/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.17500000000000002, estimator__max_depth=8, estimator__min_child_weight=9, estimator__n_estimators=7;, score=0.102 total time=  11.6s\n",
            "[CV 2/3; 12/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.17500000000000002, estimator__max_depth=8, estimator__min_child_weight=9, estimator__n_estimators=7\n",
            "[CV 2/3; 12/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.17500000000000002, estimator__max_depth=8, estimator__min_child_weight=9, estimator__n_estimators=7;, score=0.136 total time=  10.2s\n",
            "[CV 3/3; 12/12] START estimator=XGBClassifier(seed=10), estimator__eta=0.17500000000000002, estimator__max_depth=8, estimator__min_child_weight=9, estimator__n_estimators=7\n",
            "[CV 3/3; 12/12] END estimator=XGBClassifier(seed=10), estimator__eta=0.17500000000000002, estimator__max_depth=8, estimator__min_child_weight=9, estimator__n_estimators=7;, score=0.173 total time=  10.1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multilabelModel = result.best_estimator_\n",
        "multilabelModel.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmCKCTsmGUHA",
        "outputId": "d2b692b8-b1df-4fd1-8abc-ec47f19a3157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiOutputClassifier(estimator=XGBClassifier(eta=0.0, max_depth=9,\n",
              "                                              min_child_weight=6,\n",
              "                                              n_estimators=14, seed=10))"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bestValScore = multilabelModel.score(X_val, y_val)\n",
        "print(\"Mejor score de validación: \" + str(bestValScore))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj9y95gtGeXe",
        "outputId": "24cbacdf-456c-4769-ee0a-ea2ec494b71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor score de validación: 0.1653517422748192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bestTestScore = multilabelModel.score(X_test, y_test)\n",
        "print(\"Score en test: \" + str(bestTestScore))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7eh-9cODhP4",
        "outputId": "a63ab0c6-4cb3-4b8b-b6aa-7dfd908faf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score en test: 0.15163398692810456\n"
          ]
        }
      ]
    }
  ]
}